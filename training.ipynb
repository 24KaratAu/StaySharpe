{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff1e496-fa87-4eca-b263-92928dceda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54370148-45a3-4ad4-ae82-ed89ceabb3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Here are the first 5 rows:\n",
      "     open    high     low   close  volume\n",
      "0  646.90  662.00  646.90  662.00  271571\n",
      "1  661.70  662.30  653.60  653.90  193089\n",
      "2  654.20  658.00  651.05  657.95   89069\n",
      "3  657.00  657.50  655.40  656.80   68028\n",
      "4  656.95  656.95  647.00  647.00  105605\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    full_df = pd.read_csv('your_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'your_data.csv' not found\")\n",
    "    # As a fallback for testing, create a dummy dataframe\n",
    "    print(\"Creating dummy data to proceed with the example...\")\n",
    "    num_rows = 5000\n",
    "    data = {'open': np.random.uniform(98, 102, num_rows).cumsum(), 'high': 100, 'low': 100, 'close': 100, 'volume': 1000}\n",
    "    full_df = pd.DataFrame(data)\n",
    "    full_df['high'] = full_df['open'] + np.random.uniform(0, 2, num_rows)\n",
    "    full_df['low'] = full_df['open'] - np.random.uniform(0, 2, num_rows)\n",
    "    full_df['close'] = (full_df['open'] + full_df['high'] + full_df['low']) / 3\n",
    "    full_df['volume'] = np.random.randint(10000, 50000, num_rows)\n",
    "\n",
    "\n",
    "\n",
    "ohlcv_df = full_df[['open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "\n",
    "print(\"Data loaded successfully. Here are the first 5 rows:\")\n",
    "print(ohlcv_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb42edbf-7ddb-4dd7-9396-fe9d5bd3b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtrlAlpha:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # --- Hyperparameters ---\n",
    "        self.h = 8\n",
    "        self.upper_threshold = 0.002\n",
    "        self.lower_threshold = -0.002\n",
    "        self.execution_delay = 1\n",
    "\n",
    "        # --- Model & Scaler ---\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=150, max_depth=10, min_samples_leaf=10,\n",
    "            class_weight=\"balanced\",random_state=42, n_jobs=-1\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        # --- State Management ---\n",
    "        self.history_df = pd.DataFrame()\n",
    "        self.features_list = None\n",
    "        self.min_history_size = 50\n",
    "\n",
    "\n",
    "# OLD CODE\n",
    "    \n",
    "    # def _create_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    #     df_copy = df.copy()\n",
    "    #     if not isinstance(df_copy.index, pd.DatetimeIndex):\n",
    "    #          df_copy.index = pd.to_datetime(df_copy.index, unit='s')\n",
    "    #     df_copy.ta.strategy(\"common\", append=True)\n",
    "    #     df_copy.fillna(method='ffill', inplace=True)\n",
    "    #     return df_copy\n",
    "\n",
    "\n",
    "    # NEW code\n",
    "    def _create_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_copy = df.copy()\n",
    "        if not isinstance(df_copy.index, pd.DatetimeIndex):\n",
    "                df_copy.index = pd.to_datetime(df_copy.index, unit='s')\n",
    "    \n",
    "        # Manually add the indicators from the \"common\" strategy\n",
    "        df_copy.ta.rsi(append=True)\n",
    "        df_copy.ta.macd(append=True)\n",
    "        df_copy.ta.bbands(append=True)\n",
    "        df_copy.ta.obv(append=True)\n",
    "        \n",
    "        df_copy.fillna(method='ffill', inplace=True)\n",
    "        return df_copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _create_labels(self, df: pd.DataFrame) -> pd.Series:\n",
    "        forward_returns = (df['close'].shift(-self.h) / df['close']) - 1\n",
    "        conditions = [\n",
    "            (forward_returns > self.upper_threshold),\n",
    "            (forward_returns < self.lower_threshold),\n",
    "        ]\n",
    "        choices = [1, -1]\n",
    "        labels = np.select(conditions, choices, default=0)\n",
    "        labels = pd.Series(labels, index=df.index)\n",
    "    \n",
    "        # Drop last h rows (to avoid peeking into the future)\n",
    "        labels.iloc[-self.h:] = np.nan\n",
    "        return labels\n",
    "\n",
    "\n",
    "    def train(self, ohlcv_data: pd.DataFrame):\n",
    "        print(\"Starting model training...\")\n",
    "        df = ohlcv_data.copy()\n",
    "        df.columns = [col.lower() for col in df.columns] # Ensure lowercase columns\n",
    "\n",
    "        df_features = self._create_features(df)\n",
    "        df_labels = self._create_labels(df_features)\n",
    "\n",
    "        df_combined = df_features.join(df_labels.rename('signal'))\n",
    "        df_combined.dropna(inplace=True)\n",
    "\n",
    "        X = df_combined.drop(columns=['signal'])\n",
    "        y = df_combined['signal']\n",
    "\n",
    "        self.features_list = [col for col in X.columns if col in df_features.columns]\n",
    "        X = X[self.features_list]\n",
    "\n",
    "        if len(X) < self.min_history_size:\n",
    "            print(\"Not enough data to train the model.\")\n",
    "            return\n",
    "\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled, y)\n",
    "        self.history_df = df.tail(self.min_history_size * 2)\n",
    "        print(\"Training complete.\")\n",
    "\n",
    "    def predict(self, timestamp_data: pd.DataFrame) -> int:\n",
    "        df = timestamp_data.copy()\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "    \n",
    "        self.history_df = pd.concat([self.history_df, df], ignore_index=True)\n",
    "    \n",
    "        if len(self.history_df) < self.min_history_size or self.features_list is None:\n",
    "            return 0\n",
    "    \n",
    "        # Only recompute features for last min_history_size rows\n",
    "        features_df = self._create_features(self.history_df.tail(self.min_history_size).copy())\n",
    "    \n",
    "        latest_features = features_df[self.features_list].iloc[-self.execution_delay]\n",
    "    \n",
    "        if latest_features.isnull().any():\n",
    "            return 0\n",
    "    \n",
    "        scaled_features = self.scaler.transform(latest_features.values.reshape(1, -1))\n",
    "        prediction = self.model.predict(scaled_features)\n",
    "        return int(prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d4e4d9-b002-4567-badc-ce1e48d05902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 384000 rows\n",
      "Validation size: 96000 rows\n",
      "Test size: 120000 rows\n",
      "Starting model training...\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: First split (train+val vs test)\n",
    "split_point = int(len(ohlcv_df) * 0.8)\n",
    "trainval_df = ohlcv_df.iloc[:split_point]   # 80% for training+validation\n",
    "test_df = ohlcv_df.iloc[split_point:]       # 20% final test (kept untouched)\n",
    "\n",
    "# Step 2: Now split trainval into train_sub and val_sub\n",
    "split_val = int(len(trainval_df) * 0.8)\n",
    "train_sub = trainval_df.iloc[:split_val]    # ~64% of total → training\n",
    "val_sub = trainval_df.iloc[split_val:]      # ~16% of total → validation\n",
    "\n",
    "print(f\"Training size: {len(train_sub)} rows\")\n",
    "print(f\"Validation size: {len(val_sub)} rows\")\n",
    "print(f\"Test size: {len(test_df)} rows\")\n",
    "\n",
    "# Train only on training subset\n",
    "agent = CtrlAlpha()\n",
    "agent.train(train_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a562f83e-8eb3-47c4-80e4-fb7bb08590e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vectorized backtest...\n",
      "Backtest predictions complete. Here's a sample of the results:\n",
      "           open     high      low    close  volume  signal\n",
      "599900  1434.95  1436.25  1434.10  1436.25    2893     0.0\n",
      "599901  1436.25  1437.35  1435.30  1436.25    1741     0.0\n",
      "599902  1436.25  1437.35  1435.10  1435.95    4490     0.0\n",
      "599903  1435.60  1437.60  1435.60  1437.60    5004     0.0\n",
      "599904  1437.60  1437.60  1436.00  1436.50    1328     0.0\n",
      "...         ...      ...      ...      ...     ...     ...\n",
      "599995  1427.00  1429.50  1427.00  1429.25    5649     0.0\n",
      "599996  1429.35  1429.35  1427.20  1428.10    5394     0.0\n",
      "599997  1427.40  1428.00  1426.20  1426.75    5012     0.0\n",
      "599998  1426.75  1426.85  1425.20  1425.80    2440     0.0\n",
      "599999  1425.25  1425.80  1424.55  1424.90    4139     0.0\n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- The Fast, Vectorized Backtest ---\n",
    "\n",
    "print(\"Starting vectorized backtest...\")\n",
    "\n",
    "# 1. Get the last part of the training data to use as a \"lookback\" history for indicators\n",
    "lookback_period = 50 # Should be at least the longest period of your indicators\n",
    "history = train_sub.tail(lookback_period)\n",
    "\n",
    "# 2. Combine the history with the test data\n",
    "combined_df = pd.concat([history, test_df])\n",
    "\n",
    "# 3. Calculate features for the combined dataframe all at once\n",
    "# This is much faster than doing it one row at a time in a loop\n",
    "features_df = agent._create_features(combined_df.copy())\n",
    "\n",
    "# 4. Select only the features for the test period\n",
    "test_features_df = features_df.iloc[lookback_period:]\n",
    "\n",
    "# 5. Scale all the test features at once\n",
    "# Ensure we only use the columns the model was trained on\n",
    "X_test = test_features_df[agent.features_list]\n",
    "X_test_scaled = agent.scaler.transform(X_test)\n",
    "\n",
    "# 6. Predict on the entire scaled test set in one go\n",
    "# The model's predict method is vectorized and highly efficient\n",
    "all_predictions = agent.model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Assign the signals to the original test dataframe\n",
    "test_df['signal'] = all_predictions\n",
    "\n",
    "print(\"Backtest predictions complete. Here's a sample of the results:\")\n",
    "print(test_df.tail(100))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13c940d-ee1a-4367-aaed-135197341168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal Distribution in the Entire Test Set:\n",
      "signal\n",
      " 0.0    61774\n",
      "-1.0    33260\n",
      " 1.0    24966\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Signal Distribution (%):\n",
      "signal\n",
      " 0.0    51.478333\n",
      "-1.0    27.716667\n",
      " 1.0    20.805000\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- In a new cell ---\n",
    "\n",
    "# Check the value counts of all signals in the test set\n",
    "print(\"Signal Distribution in the Entire Test Set:\")\n",
    "print(test_df['signal'].value_counts())\n",
    "\n",
    "# Check the distribution as a percentage\n",
    "print(\"\\nSignal Distribution (%):\")\n",
    "print(test_df['signal'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d5ef8c-fb5a-4a78-bcae-2878290ed6d3",
   "metadata": {},
   "source": [
    "## OPTIMIZING HYPERPARAMETERS THRU GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1f9b81b-68a6-479a-953e-0d9f90c34d6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Coarse Grid Search ---\n",
      "--- Testing h=10, threshold=0.0020 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.34, Max Drawdown: -35.61%. Took 12.61s\n",
      "--- Testing h=10, threshold=0.0060 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.04, Max Drawdown: -35.70%. Took 13.02s\n",
      "--- Testing h=10, threshold=0.0100 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: -0.14, Max Drawdown: -79.75%. Took 13.47s\n",
      "--- Testing h=30, threshold=0.0020 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.35, Max Drawdown: -32.23%. Took 11.67s\n",
      "--- Testing h=30, threshold=0.0060 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.08, Max Drawdown: -51.65%. Took 12.78s\n",
      "--- Testing h=30, threshold=0.0100 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: -0.08, Max Drawdown: -66.89%. Took 12.29s\n",
      "--- Testing h=50, threshold=0.0020 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.18, Max Drawdown: -30.21%. Took 11.45s\n",
      "--- Testing h=50, threshold=0.0060 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.06, Max Drawdown: -44.00%. Took 11.42s\n",
      "--- Testing h=50, threshold=0.0100 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: -0.09, Max Drawdown: -71.51%. Took 11.42s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "h_values_coarse = [10, 30, 50]\n",
    "threshold_values_coarse = [0.002, 0.006, 0.010]\n",
    "\n",
    "results_list_coarse = []\n",
    "print(\"--- Starting Coarse Grid Search ---\")\n",
    "\n",
    "# Looping through every combination\n",
    "for h in h_values_coarse:\n",
    "    for threshold in threshold_values_coarse:\n",
    "        start_time = time.time()\n",
    "        print(f\"--- Testing h={h}, threshold={threshold:.4f} ---\")\n",
    "\n",
    "        # Configure and train the agent for this specific loop\n",
    "        agent = CtrlAlpha()\n",
    "        agent.h = h\n",
    "        agent.upper_threshold = threshold\n",
    "        agent.lower_threshold = -threshold\n",
    "        agent.train(train_sub) # Train only on the training subset\n",
    "\n",
    "        # Backtest on the validation subset\n",
    "        history = train_sub.tail(50)\n",
    "        combined_df = pd.concat([history, val_sub])\n",
    "        features_df = agent._create_features(combined_df.copy())\n",
    "        test_features_df = features_df.iloc[50:]\n",
    "        \n",
    "        if agent.features_list is None:\n",
    "            print(\"Agent not trained properly, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        X_val = test_features_df[agent.features_list]\n",
    "        X_val_scaled = agent.scaler.transform(X_val)\n",
    "        predictions = agent.model.predict(X_val_scaled)\n",
    "        \n",
    "        val_df = val_sub.copy()\n",
    "        val_df['signal'] = predictions\n",
    "        \n",
    "        # Calculate metrics\n",
    "        val_df['daily_return'] = val_df['close'].pct_change()\n",
    "        val_df['strategy_return'] = val_df['daily_return'] * val_df['signal'].shift(1)\n",
    "        \n",
    "        if val_df['strategy_return'].std() == 0:\n",
    "            sharpe_ratio = 0\n",
    "        else:\n",
    "            sharpe_ratio = (val_df['strategy_return'].mean() / val_df['strategy_return'].std()) * np.sqrt(252)\n",
    "        \n",
    "        cumulative_returns = (1 + val_df['strategy_return']).cumprod()\n",
    "        peak = cumulative_returns.expanding(min_periods=1).max()\n",
    "        drawdown = (cumulative_returns / peak) - 1\n",
    "        max_drawdown = drawdown.min()\n",
    "\n",
    "        # Store the results\n",
    "        results_list_coarse.append({\n",
    "            'h': h,\n",
    "            'threshold': threshold,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'max_drawdown': max_drawdown\n",
    "        })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Done. Sharpe: {sharpe_ratio:.2f}, Max Drawdown: {max_drawdown:.2%}. Took {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac14e695-ebad-4ac3-ad83-702586953be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Coarse Grid Search Results ---\n",
      "    h  threshold  sharpe_ratio  max_drawdown\n",
      "3  30      0.002      0.348246     -0.322272\n",
      "0  10      0.002      0.344580     -0.356060\n",
      "6  50      0.002      0.180634     -0.302063\n",
      "4  30      0.006      0.082864     -0.516548\n",
      "7  50      0.006      0.063012     -0.439951\n",
      "1  10      0.006      0.043635     -0.356975\n",
      "5  30      0.010     -0.080290     -0.668892\n",
      "8  50      0.010     -0.092336     -0.715120\n",
      "2  10      0.010     -0.139960     -0.797539\n"
     ]
    }
   ],
   "source": [
    "results_df_coarse = pd.DataFrame(results_list_coarse)\n",
    "\n",
    "print(\"\\n--- Coarse Grid Search Results ---\")\n",
    "print(results_df_coarse.sort_values(by='sharpe_ratio', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57f5e74c-8e73-47da-8e06-416e309135f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fine-Grained Grid Search ---\n",
      "--- Testing h=20, threshold=0.0010 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.35, Max Drawdown: -33.76%. Took 11.74s\n",
      "--- Testing h=20, threshold=0.0020 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.33, Max Drawdown: -30.44%. Took 11.46s\n",
      "--- Testing h=20, threshold=0.0030 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.35, Max Drawdown: -29.15%. Took 11.55s\n",
      "--- Testing h=30, threshold=0.0010 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.34, Max Drawdown: -34.13%. Took 11.71s\n",
      "--- Testing h=30, threshold=0.0020 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.35, Max Drawdown: -32.23%. Took 11.55s\n",
      "--- Testing h=30, threshold=0.0030 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.34, Max Drawdown: -29.11%. Took 11.89s\n",
      "--- Testing h=40, threshold=0.0010 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.30, Max Drawdown: -28.71%. Took 12.60s\n",
      "--- Testing h=40, threshold=0.0020 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.31, Max Drawdown: -26.86%. Took 12.78s\n",
      "--- Testing h=40, threshold=0.0030 ---\n",
      "Starting model training...\n",
      "Training complete.\n",
      "Done. Sharpe: 0.26, Max Drawdown: -29.31%. Took 12.44s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define a TIGHT grid around the best coarse results (h=30, threshold=0.002)\n",
    "h_values_fine = [20, 30, 40]\n",
    "threshold_values_fine = [0.001, 0.002, 0.003]\n",
    "\n",
    "results_list_fine = []\n",
    "print(\"--- Starting Fine-Grained Grid Search ---\")\n",
    "\n",
    "# The rest of the loop is identical to the one above\n",
    "for h in h_values_fine:\n",
    "    for threshold in threshold_values_fine:\n",
    "        start_time = time.time()\n",
    "        print(f\"--- Testing h={h}, threshold={threshold:.4f} ---\")\n",
    "\n",
    "        agent = CtrlAlpha()\n",
    "        agent.h = h\n",
    "        agent.upper_threshold = threshold\n",
    "        agent.lower_threshold = -threshold\n",
    "        agent.train(train_sub)\n",
    "\n",
    "        history = train_sub.tail(50)\n",
    "        combined_df = pd.concat([history, val_sub])\n",
    "        features_df = agent._create_features(combined_df.copy())\n",
    "        test_features_df = features_df.iloc[50:]\n",
    "        \n",
    "        if agent.features_list is None:\n",
    "            print(\"Agent not trained properly, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        X_val = test_features_df[agent.features_list]\n",
    "        X_val_scaled = agent.scaler.transform(X_val)\n",
    "        predictions = agent.model.predict(X_val_scaled)\n",
    "        \n",
    "        val_df = val_sub.copy()\n",
    "        val_df['signal'] = predictions\n",
    "        \n",
    "        val_df['daily_return'] = val_df['close'].pct_change()\n",
    "        val_df['strategy_return'] = val_df['daily_return'] * val_df['signal'].shift(1)\n",
    "        \n",
    "        if val_df['strategy_return'].std() == 0:\n",
    "            sharpe_ratio = 0\n",
    "        else:\n",
    "            sharpe_ratio = (val_df['strategy_return'].mean() / val_df['strategy_return'].std()) * np.sqrt(252)\n",
    "\n",
    "        cumulative_returns = (1 + val_df['strategy_return']).cumprod()\n",
    "        peak = cumulative_returns.expanding(min_periods=1).max()\n",
    "        drawdown = (cumulative_returns / peak) - 1\n",
    "        max_drawdown = drawdown.min()\n",
    "\n",
    "        results_list_fine.append({\n",
    "            'h': h,\n",
    "            'threshold': threshold,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'max_drawdown': max_drawdown\n",
    "        })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Done. Sharpe: {sharpe_ratio:.2f}, Max Drawdown: {max_drawdown:.2%}. Took {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c257d6-422e-44ff-8002-67b834c6d29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Grained Search Results ---\n",
      "    h  threshold  sharpe_ratio  max_drawdown\n",
      "0  20      0.001      0.349375     -0.337570\n",
      "2  20      0.003      0.348494     -0.291536\n",
      "4  30      0.002      0.348246     -0.322272\n",
      "3  30      0.001      0.344823     -0.341333\n",
      "5  30      0.003      0.335467     -0.291136\n",
      "1  20      0.002      0.325798     -0.304360\n",
      "7  40      0.002      0.305865     -0.268636\n",
      "6  40      0.001      0.298608     -0.287118\n",
      "8  40      0.003      0.259260     -0.293106\n"
     ]
    }
   ],
   "source": [
    "results_df_fine = pd.DataFrame(results_list_fine)\n",
    "\n",
    "print(\"\\n Fine-Grained Search Results: \")\n",
    "print(results_df_fine.sort_values(by='sharpe_ratio', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cf5e0b2-b4ab-4179-8fcc-3a61a80a5f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal Distribution in the Entire Test Set:\n",
      "signal\n",
      " 0.0    61774\n",
      "-1.0    33260\n",
      " 1.0    24966\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Signal Distribution (%):\n",
      "signal\n",
      " 0.0    51.478333\n",
      "-1.0    27.716667\n",
      " 1.0    20.805000\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "-\n",
    "\n",
    "\n",
    "print(\"Signal Distribution in the Entire Test Set:\")\n",
    "print(test_df['signal'].value_counts())\n",
    "\n",
    "\n",
    "print(\"\\nSignal Distribution (%):\")\n",
    "print(test_df['signal'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b5ac21a-69e4-4b74-ba12-aa0ab42a3740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3981\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.34      0.31      0.32     36423\n",
      "         0.0       0.44      0.61      0.51     44220\n",
      "         1.0       0.39      0.25      0.30     39317\n",
      "\n",
      "    accuracy                           0.40    119960\n",
      "   macro avg       0.39      0.39      0.38    119960\n",
      "weighted avg       0.39      0.40      0.38    119960\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11243 17553  7627]\n",
      " [ 9637 26881  7702]\n",
      " [12376 17305  9636]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1. Generate the true labels for the test set using the same method as in training\n",
    "# We use the agent's configured h and thresholds\n",
    "y_true_series = agent._create_labels(test_df.copy())\n",
    "\n",
    "# 2. Get the predictions your model made\n",
    "y_pred_series = test_df['signal']\n",
    "\n",
    "# 3. Align the true labels and predictions\n",
    "# We create a temporary DataFrame and drop rows where the true label is NaN \n",
    "# (the last h rows of the test set)\n",
    "report_df = pd.DataFrame({'y_true': y_true_series, 'y_pred': y_pred_series})\n",
    "report_df.dropna(inplace=True)\n",
    "\n",
    "y_true = report_df['y_true']\n",
    "y_pred = report_df['y_pred']\n",
    "\n",
    "# 4. Calculate and print all the metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e3aab-6119-437f-a71b-ef42e6545278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctrl",
   "language": "python",
   "name": "hackathon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
